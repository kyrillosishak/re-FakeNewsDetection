{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyrillosishak/re-FakeNewsDetection/blob/main/notebooks/TemporalLeakage_ToyExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "234e6db7-e412-4eb8-bfd4-52e2a4a881ff"
      },
      "source": [
        "# Temporal Leakage"
      ],
      "id": "234e6db7-e412-4eb8-bfd4-52e2a4a881ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4e922c0-0b9e-4f7f-af94-d4b4b3ab27dd"
      },
      "source": [
        "Time series data is a collection of data points arranged in chronological order. This type of data is common across various fields, such as finance, where stock prices fluctuate over time, and weather forecasting, where temperature readings are recorded in sequence.\n",
        "\n",
        "<img src=\"https://github.com/kyrillosishak/re-FakeNewsDetection/raw/main/assets/TimeSeries_DataLeaked.png\" height = 300>\n",
        "\n",
        "Imagine you have 1,000 samples of Microsoft’s (MSFT) stock price throughout a single day. You might be tempted to randomly select 500 samples for your training set and reserve the rest for validation. However, this approach can make your model appear astonishingly accurate, as though it possesses an uncanny ability to predict Microsoft’s future stock prices. The reason? Your model has inadvertently seen the future—the training set includes data from both before and after nearly every point in the validation set. From the perspective of the validation data, information from the future has leaked into your model, undermining the integrity of your evaluation.\n",
        "\n",
        "To illustrate the idea consider this Scenario:\n",
        "\n",
        "-   In which an e-commerce platform collects data on users, including their profiles, purchase histories, and product ratings. The goal is to demonstrate how data leakage can occur when building a machine learning model to predict product ratings.\n",
        "\n",
        "-   In this scenario, the platform has data on users’ ages, gift card balances, and purchase quantities over time. Additionally, each user has given ratings to certain products.\n",
        "\n",
        "-   *we will create synthetic datasets for user profiles, purchase histories, and product ratings*.\n",
        "\n",
        "-   *The key point of the scenario is that the purchase quantities are time-dependent features, meaning they change over time. When trying to predict product ratings using these features, it is crucial to ensure that the model does not have access to information from the future, such as purchases made after the rating was given.*\n",
        "\n",
        "Then we create two different datasets:\n",
        "\n",
        "1.  With Leakage: This dataset includes purchase information from both before and after the rating was given, leading to potential leakage of future information into the model.\n",
        "2.  Without Leakage: This dataset carefully removes any purchase data that occurs after the rating was given, ensuring no future information is leaked into the model."
      ],
      "id": "c4e922c0-0b9e-4f7f-af94-d4b4b3ab27dd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ec19032-7cf2-46b9-a2de-0ef06205a712"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate 1000 user profiles\n",
        "n_samples = 50000\n",
        "user_ids = np.arange(1, n_samples + 1)\n",
        "ages = np.random.randint(18, 70, size=n_samples)\n",
        "gift_card_balances = np.random.randint(0, 2000, size=n_samples)\n",
        "\n",
        "user_profile_data = pd.DataFrame({\n",
        "    'user_id': user_ids,\n",
        "    'age': ages,\n",
        "    'gift_card_balance': gift_card_balances\n",
        "})\n",
        "\n",
        "# Generate purchase history with multiple entries per user\n",
        "purchase_times = pd.date_range(start='2022-01-01', end='2022-12-31', periods=n_samples*3)\n",
        "purchase_quantities = np.random.randint(1, 300, size=n_samples*3)\n",
        "purchase_user_ids = np.random.choice(user_ids, size=n_samples*3)\n",
        "\n",
        "user_purchase_history = pd.DataFrame({\n",
        "    'user_id': purchase_user_ids,\n",
        "    'purchase_time': np.random.choice(purchase_times, size=n_samples*3),\n",
        "    'purchase_quantity': purchase_quantities\n",
        "})\n",
        "\n",
        "# Generate observation data (product ratings)\n",
        "event_times = pd.date_range(start='2022-01-01', end='2022-12-31', periods=n_samples)\n",
        "# Introduce a strong correlation between future purchases and product rating\n",
        "product_ratings = np.random.randint(1, 6, size=n_samples) + \\\n",
        "                  np.array([np.sum(user_purchase_history[\n",
        "                    (user_purchase_history['user_id'] == user_id) &\n",
        "                    (user_purchase_history['purchase_time'] > event_time)]['purchase_quantity']) / 100\n",
        "                    for user_id, event_time in zip(user_ids, event_times)])\n",
        "\n",
        "# Clip ratings to be between 1 and 5\n",
        "product_ratings = np.clip(product_ratings, 1, 5)\n",
        "\n",
        "user_observation_data = pd.DataFrame({\n",
        "    'user_id': user_ids,\n",
        "    'event_time': event_times,\n",
        "    'Product_rating': product_ratings\n",
        "})\n"
      ],
      "id": "9ec19032-7cf2-46b9-a2de-0ef06205a712"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77798170-1613-4525-a2f8-ad827d286dc0"
      },
      "outputs": [],
      "source": [
        "# Merge user profile data with observation data\n",
        "merged_data = pd.merge(user_observation_data, user_profile_data, on='user_id')\n",
        "\n",
        "# Introduce potential leakage by joining purchase history without time restriction\n",
        "merged_data_leak = pd.merge(merged_data, user_purchase_history, on='user_id', how='left')\n",
        "\n",
        "# Filter to remove future purchases in no-leakage scenario\n",
        "merged_data_no_leak = merged_data_leak[merged_data_leak['purchase_time'] <= merged_data_leak['event_time']]\n",
        "\n",
        "# Aggregate purchase quantities for no-leakage scenario\n",
        "agg_data_no_leak = merged_data_no_leak.groupby(['user_id', 'event_time', 'age', 'gift_card_balance']).agg({\n",
        "    'purchase_quantity': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Aggregate purchase quantities for leakage scenario (includes future data)\n",
        "agg_data_leak = merged_data_leak.groupby(['user_id', 'event_time', 'age', 'gift_card_balance']).agg({\n",
        "    'purchase_quantity': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Merge with the original ratings\n",
        "agg_data_no_leak = pd.merge(agg_data_no_leak, user_observation_data, on=['user_id', 'event_time'])\n",
        "agg_data_leak = pd.merge(agg_data_leak, user_observation_data, on=['user_id', 'event_time'])\n",
        "\n",
        "# Extract features and labels\n",
        "X_leak = agg_data_leak[['age', 'gift_card_balance', 'purchase_quantity']]\n",
        "y_leak = agg_data_leak['Product_rating']\n",
        "\n",
        "X_no_leak = agg_data_no_leak[['age', 'gift_card_balance', 'purchase_quantity']]\n",
        "y_no_leak = agg_data_no_leak['Product_rating']\n"
      ],
      "id": "77798170-1613-4525-a2f8-ad827d286dc0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef7bc613-a988-4251-89a5-7a26dd744f0b"
      },
      "source": [
        "Then we train two linear regression models, one on each dataset, and evaluates their performance using Mean Squared Error (MSE). The results highlight how data leakage can lead to misleadingly good model performance during training, which may not translate to real-world accuracy. This underscores the importance of handling time-dependent features carefully to avoid introducing bias into the model."
      ],
      "id": "ef7bc613-a988-4251-89a5-7a26dd744f0b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8a6465e-6d25-4244-ae7a-2c7bb3bb9902"
      },
      "outputs": [],
      "source": [
        "# Initialize Linear Regression models\n",
        "model_leak = LinearRegression()\n",
        "model_no_leak = LinearRegression()\n",
        "\n",
        "# Train the model with leakage\n",
        "model_leak.fit(X_leak, y_leak)\n",
        "\n",
        "# Train the model without leakage\n",
        "model_no_leak.fit(X_no_leak, y_no_leak)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_leak = model_leak.predict(X_leak)\n",
        "y_pred_no_leak = model_no_leak.predict(X_no_leak)\n",
        "\n",
        "# Evaluate models\n",
        "mse_leak = mean_squared_error(y_leak, y_pred_leak)\n",
        "mse_no_leak = mean_squared_error(y_no_leak, y_pred_no_leak)\n",
        "\n",
        "print(f'MSE with Leakage: {mse_leak}')\n",
        "print(f'MSE without Leakage: {mse_no_leak}')"
      ],
      "id": "a8a6465e-6d25-4244-ae7a-2c7bb3bb9902"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  }
}